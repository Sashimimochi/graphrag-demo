import os
import numpy as np
import streamlit as st
import streamlit.components.v1 as components
from dotenv import load_dotenv
from lightrag import LightRAG, QueryParam
from lightrag.utils import EmbeddingFunc
from lightrag.llm import openai_embedding, openai_complete_if_cache
from utils.graph_visualize import visualize_graphml, show_hierarchy_graph
from neo4j import GraphDatabase

# ç’°å¢ƒå¤‰æ•°ã‚’ãƒ­ãƒ¼ãƒ‰
load_dotenv()

# å®šæ•°ã®è¨­å®š
DATA_DIR = "./data"
API_KEY = os.getenv("API_KEY")
BASE_URL = "https://generativelanguage.googleapis.com/v1beta/openai"

# Streamlitã®ãƒšãƒ¼ã‚¸è¨­å®š
def configure_page():
    st.set_page_config(
        page_title="GraphRAG Demo",
        page_icon="ğŸ§Š",
        layout="wide",
        initial_sidebar_state="expanded",
        menu_items={
            'Get Help': 'https://docs.streamlit.io/',
            'Report a bug': "https://docs.streamlit.io/",
            'About': "# This is a header. This is an *extremely* cool app!"
        }
    )

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
def initialize_session_state():
    if "conversation" not in st.session_state:
        st.session_state.conversation = []

# LLMãƒ¢ãƒ‡ãƒ«é–¢æ•°ã®å®šç¾©
async def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs) -> str:
    return await openai_complete_if_cache(
        "gemini-1.5-flash",
        prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        api_key=API_KEY,
        base_url=BASE_URL,
        **kwargs
    )

# åŸ‹ã‚è¾¼ã¿é–¢æ•°ã®å®šç¾©
async def embedding_func(texts: list[str]) -> np.ndarray:
    return await openai_embedding(
        texts,
        model="text-embedding-004",
        api_key=API_KEY,
        base_url=BASE_URL,
    )

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆé–¢æ•°ã®å®šç¾©
def make_index(filepath):
    with open(os.path.join(DATA_DIR, f"{filepath}.txt")) as f:
        st.session_state.rag.insert(f.read())
    st.success("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

# æ¤œç´¢é–¢æ•°ã®å®šç¾©
def search(mode, query="ã“ã®æ–‡ç« ã‚’èª­ã‚€ã¨ã©ã®ã‚ˆã†ãªçŸ¥è¦‹ãŒå¾—ã‚‰ã‚Œã‚‹ã‹ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚"):
    """
    Perform mix search (Knowledge Graph + Vector Retrieval)
    Mix mode combines knowledge graph and vector search:
    - Uses both structured (KG) and unstructured (vector) information
    - Provides comprehensive answers by analyzing relationships and context
    - Supports image content through HTML img tags
    - Allows control over retrieval depth via top_k parameter
    """
    msg = st.session_state.rag.query(query, param=QueryParam(mode=mode))
    print("="*100)
    print(f"\nquestion: {query}")
    print(f"[Mode: {mode} Search]")
    print("-"*30)
    print("")
    print(msg)
    return msg

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®é¸æŠ
def select_dataset():
    return st.text_input(
        "Select Data",
        help="dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«äº‹å‰ã«ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§åŒåã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€`dickens`ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚",
        placeholder="dickens"
    )

# è¨€èªã®é¸æŠ
def select_language():
    return st.selectbox(
        "Select Language",
        ["Japanese", "English"],
        help="æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ/è³ªå•ã‚’ã™ã‚‹å ´åˆã¯`Japanese`ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"
    )

# ã‚°ãƒ©ãƒ•ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®é¸æŠ
def select_graph_storage():
    return "Neo4JStorage" if os.getenv("NEO4J_URI") else "NetworkXStorage"

# Neo4jã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®æ¥ç¶šç¢ºèª
def verify_neo4j_connection():
    neo4j_uri = os.getenv('NEO4J_URI')
    username = os.getenv('NEO4J_USERNAME')
    password = os.getenv('NEO4J_PASSWORD')
    with GraphDatabase.driver(neo4j_uri, auth=(username, password)) as driver:
        driver.verify_connectivity()
        st.success("Successfully access neo4j")

# RAGã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®åˆæœŸåŒ–
def initialize_rag(working_dir, language, graph_storage):
    if "rag" not in st.session_state:
        rag = LightRAG(
            working_dir=working_dir,
            llm_model_func=llm_model_func,
            llm_model_max_async=1,
            embedding_func=EmbeddingFunc(
                embedding_dim=768,
                max_token_size=4096,
                func=embedding_func
            ),
            graph_storage=graph_storage,
            addon_params={
                "language": language,
                "entity_types": ["organization", "person", "geo", "event", "category", "product"],
            },
        )
        st.session_state.rag = rag

# æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã®é¸æŠ
def select_search_mode():
    return st.selectbox(
        "Select Search Mode",
        ["naive", "local", "global", "hybrid", "mix"],
        key="mode",
        help="- `naive`: å˜ç´”ãªé¡ä¼¼æ¤œç´¢\n- `local`: äººç‰©ç›¸é–¢ãªã©ç‰¹å®šã®é–¢ä¿‚æ€§ã«ã¤ã„ã¦è³ªå•ã‚’ã™ã‚‹\n- `global`: æ–‡ç« å…¨ä½“ã«ã¾ãŸãŒã‚‹æŠ½è±¡çš„ãªè³ªå•ã‚’ã™ã‚‹\n- `hybrid`: `local`ã¨`global`ã®ä¸¡æ–¹ã‚’æ··ãœãŸã‚‚ã®\n- `mix`: çŸ¥è­˜ã‚°ãƒ©ãƒ•ã¨ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’çµ„ã¿åˆã‚ã›ã¦æ¤œç´¢ã™ã‚‹"
    )

# çŸ¥è­˜ã‚°ãƒ©ãƒ•ã®è¡¨ç¤º
def display_knowledge_graph(graph_storage, filename):
    if graph_storage == "Neo4JStorage":
        st.markdown(f"""
            <a href="{os.getenv("NEO4J_BROWSER_URI")}" target="_blank">
                Neo4j
            </a>
        """, unsafe_allow_html=True)
    else:
        filepath = f"./visualize/knowledge_graph_{filename}.html"
        if not os.path.exists(filepath):
            visualize_graphml(filename, filepath)
        with open(filepath, "r") as f:
            components.html(f.read(), height=500)
        df = show_hierarchy_graph(filename)
        st.dataframe(df)

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
def initialize_chat_history():
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {
                "role": "assistant",
                "content": "ç§ã¯ãŠåŠ©ã‘Botã§ã™ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ãŒã‚ã‚Œã°èã„ã¦ãã ã•ã„ã€‚"
            }
        ]

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
def display_chat_history():
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«åå¿œ
def handle_user_input(mode):
    if prompt := st.chat_input("LLMã¸ã®è³ªå•å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ex.ã“ã®æ–‡ç« ã®ä¸»è¦ãªãƒ†ãƒ¼ãƒã¯ãªã‚“ã§ã™ã‹ï¼Ÿ", key="query"):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º
        st.chat_message("user").markdown(prompt)
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
        st.session_state.messages.append({"role": "user", "content": prompt})
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º
        with st.chat_message("assistant"):
            with st.spinner("Assistant is thinking..."):
                placeholder = st.empty()
                msg = search(mode, prompt)
                placeholder.markdown(msg)
                # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
                st.session_state.messages.append({"role": "assistant", "content": msg})

# ãƒ¡ã‚¤ãƒ³é–¢æ•°ã®å®šç¾©
def main():
    configure_page()
    initialize_session_state()

    st.title("GraphRAG Demo")
    st.write("GraphRAGã®å¨åŠ›ã‚’ä½“æ„Ÿã§ãã‚‹ãƒ‡ãƒ¢ã‚¢ãƒ—ãƒªã§ã™ã€‚")

    DATASET = select_dataset()
    WORKING_DIR = DATASET

    if not WORKING_DIR:
        return

    LANGUAGE = select_language()
    filename = DATASET

    if not os.path.exists(WORKING_DIR):
        os.mkdir(WORKING_DIR)
        st.warning(f"`data`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã«`{filename}.txt`ã‚’é…ç½®ã—ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")

    graph_storage = select_graph_storage()
    st.info(f"Using Graph Storage: {graph_storage}")

    if graph_storage == "Neo4JStorage":
        verify_neo4j_connection()

    initialize_rag(WORKING_DIR, LANGUAGE, graph_storage)

    mode = select_search_mode()

    if st.button("Create Index", help="åˆã‚ã¦ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯ã€è³ªå•ã®å‰ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"):
        with st.spinner("Creating index..."):
            make_index(filename)

    if st.button("View Knowledge Graph", help="çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’ç¢ºèªã™ã‚‹"):
        display_knowledge_graph(graph_storage, filename)

    initialize_chat_history()
    display_chat_history()
    handle_user_input(mode)

# ãƒ¡ã‚¤ãƒ³é–¢æ•°ã®å®Ÿè¡Œ
if __name__ == "__main__":
    main()
